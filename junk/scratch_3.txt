Stochastic gradient descent with random batch selection based on selection and active coount
active / selection + dynamic epsilon
Add unit test for different modes of operation
Make functions composable for jax/jit
Sort data if animation = "all"

getters: dFdA, dFdE, A, E
setters: dFdA, dFdE, A, E

substiture self.m in compute function
mask using boolean same class matrics (benchmark)
unfunc in_place, out, where, += -= *=
set and get functions for interacting with MLNNEngine
Scalars are python floats
__init__ and optimize should not need JAX
grid search unit test

func v grad shared computation
base class
intr(self, X)
func(self, X, I=None, full_output=False)
grad(self, X, I=None, full_output=False)
_intr(X, params)
_func(I, params)
_grad(I, params)


kernel=None | None, 'rbf', 'linear'
regularization='auto' | 'auto', 'unweighted'
init='auto' | 'auto', 'random', 'zero', 'identity', 'centered', 'pca', 'kpca'
callback=None | None, f(MLNNEngine, MLNNOptimizer, iter)
solver='steepest_fixed_backtracking' | 'steepest'/'bfgs', 'fixed'/'alternating', 'backtracking'/'strong_wolfe'
backend='numpy' | 'numpy', 'jax', 'cupy', 'numba'
verbose=0 | 0, 1, 2

Expand[Piecewise[{{0, x <= -1 / 2}, {2 / 3 * (x + 1 / 2) ^ 3, -1 / 2 < x <= 0}, {-2 / 3 * (x + 1 / 2) ^ 3 + 2 * (x + 1 / 2) ^ 2 - x - 1 / 3, 0 < x <= 1 / 2}, {x, 1 / 2 < x}}]]
Expand[D[Piecewise[{{0, x <= -1 / 2}, {2 / 3 * (x + 1 / 2) ^ 3, -1 / 2 < x <= 0}, {-2 / 3 * (x + 1 / 2) ^ 3 + 2 * (x + 1 / 2) ^ 2 - x - 1 / 3, 0 < x <= 1 / 2}, {x, 1 / 2 < x}}], x]]
