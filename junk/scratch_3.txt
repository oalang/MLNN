Stochastic gradient descent with random batch selection based on selection and active coount
active / selection + dynamic epsilon
Add unit test for different modes of opperation
Fixed length active set vectors
Make functions composable for jax/jit

func v grad shared computation
base class
intr(self, X)
func(self, X, I=None, full_output=False)
grad(self, X, I=None, full_output=False)
_intr(X, params)
_func(I, params)
_grad(I, params)


kernel=None | None, 'rbf', 'linear'
regularization='auto' | 'auto', 'unweighted'
init='auto' | 'auto', 'random', 'zero', 'identity', 'centered', 'pca', 'kpca'
callback=None | None, f(MLNNEngine, MLNNOptimizer, iter)
solver='steepest_fixed_backtracking' | 'steepest'/'bfgs', 'fixed'/'alternating', 'backtracking'/'strong_wolfe'
backend='numpy' | 'numpy', 'jax', 'cupy', 'numba'
verbose=0 | 0, 1, 2

Expand[Piecewise[{{0, x <= -1 / 2}, {2 / 3 * (x + 1 / 2) ^ 3, -1 / 2 < x <= 0}, {-2 / 3 * (x + 1 / 2) ^ 3 + 2 * (x + 1 / 2) ^ 2 - x - 1 / 3, 0 < x <= 1 / 2}, {x, 1 / 2 < x}}]]
Expand[D[Piecewise[{{0, x <= -1 / 2}, {2 / 3 * (x + 1 / 2) ^ 3, -1 / 2 < x <= 0}, {-2 / 3 * (x + 1 / 2) ^ 3 + 2 * (x + 1 / 2) ^ 2 - x - 1 / 3, 0 < x <= 1 / 2}, {x, 1 / 2 < x}}], x]]

(A > 0).astype(float)

class LeakySmoothReLU2(Base):
    def __init__(self, offset=0, alpha=1e-2):
        assert 0 <= alpha <= 0.5

        self.params = {
            'offset': offset,
            'alpha': alpha,
            'a': np.sqrt(0.5 * alpha),
            'b': 0.5 * alpha - np.sqrt(2) / 3 * alpha ** (3 / 2),
        }

    @staticmethod
    def _intr(X, params):
        offset = params['offset']

        A = X + offset
        B = np.square(A)
        return (A, B)

    @staticmethod
    def _func(I, params):
        alpha = params['alpha']
        a = params['a']
        b = params['b']

        A = I[0]
        B = I[1]
        C = 2 / 3 * B * A
        D = B + A / 2 + 1 / 12
        F = np.where(A > 0.5, A,
                     np.where(A > 0, D - C,
                              np.where(A > a, D + C, alpha * A + b)))
        return F

    @staticmethod
    def _grad(I, params):
        alpha = params['alpha']
        a = params['a']

        A = I[0]
        B = I[1]
        C = 2 * B
        D = 2 * A + 1 / 2
        G = np.where(A > 0.5, 1,
                     np.where(A > 0, D - C,
                              np.where(A > a, D + C, alpha)))
        return G