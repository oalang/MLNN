Stochastic gradient descent with random batch selection based on selection and active coount
active / selection + dynamic epsilon
Add unit test for different modes of operation
Make functions composable for jax/jit
Sort data if animation = "all"
Return zeros array and boolean
Remove r and s gating
Reorder everything

A : m x m, m x 1, d x m

jax.lax.cond
K = empty if r is 0

dF = zeros, if ... +=, if ... +=

get rid of outer loss if statement, precompute if None

static variables
(T, N, compute_Q)
intermediate variables module
(J, K, I, O) (D)
F module
R, S, L) (F)
dF module
U, phiA, phiE), (dFdA, dFdE)

getters: dFdA, dFdE, A, E (only if converting to NumPy)
setters: dFdA, dFdE, A, E (only if converting from NumPy)

X
Y
A     getter+setter
E     getter+setter
dFdA  getter+setter
dFdE  getter+setter
D
U
active_rows and/or active_rows_count
active_cols and/or active_cols_count
active_data and/or active_data_count
V get_transformation_matrix() - self.A.shape[0]?

* _A_psd_projection
* _A_centered_projection
* _E_positive_projection
* update_A
* update_E
  compute_A_0
  compute_E_0
* fun
* jac
* get_transformation_matrix (V) - self.A.shape[0]?
* get X transformed n_components <- static


substiture self.m in compute function
mask using boolean same class matrics (benchmark)
unfunc in_place, out, where, += -= *=
set and get functions for interacting with MLNNEngine
Scalars are python floats
__init__ and optimize should not need JAX
grid search unit test

func v grad shared computation
base class
intr(self, X)
func(self, X, I=None, full_output=False)
grad(self, X, I=None, full_output=False)
_intr(X, params)
_func(I, params)
_grad(I, params)


kernel=None | None, 'rbf', 'linear'
regularization='auto' | 'auto', 'unweighted'
init='auto' | 'auto', 'random', 'zero', 'identity', 'centered', 'pca', 'kpca'
callback=None | None, f(MLNNEngine, MLNNOptimizer, iter)
solver='steepest_fixed_backtracking' | 'steepest'/'bfgs', 'fixed'/'alternating', 'backtracking'/'strong_wolfe'
backend='numpy' | 'numpy', 'jax', 'cupy', 'numba'
verbose=0 | 0, 1, 2

Expand[Piecewise[{{0, x <= -1 / 2}, {2 / 3 * (x + 1 / 2) ^ 3, -1 / 2 < x <= 0}, {-2 / 3 * (x + 1 / 2) ^ 3 + 2 * (x + 1 / 2) ^ 2 - x - 1 / 3, 0 < x <= 1 / 2}, {x, 1 / 2 < x}}]]
Expand[D[Piecewise[{{0, x <= -1 / 2}, {2 / 3 * (x + 1 / 2) ^ 3, -1 / 2 < x <= 0}, {-2 / 3 * (x + 1 / 2) ^ 3 + 2 * (x + 1 / 2) ^ 2 - x - 1 / 3, 0 < x <= 1 / 2}, {x, 1 / 2 < x}}], x]]

(shared) (secondary) (primary)
(J, K, I, O) (D, R, S, L) (F)
(J, K, I, O) (D, U, phiA, phiE), (dFdA, dFdE)

Reorder everything


q = 2.3
Y 10000
D 10000 x 10000
E 1 x 1

T = np.where(np.equal.outer(Y, Y), 1, -1)
N = q * (np.sum(T == 1, axis=1, keepdims=True) - 1)
Q = np.where(T == 1, q, 1)

I = (D - E) * T

I *= Q



###################

Good call. When U isn’t symmetric, you can still avoid materializing W and W @ X.

Let W0 = -(U + U.T) and s = sum(W0, axis=0) = -(U.sum(axis=0) + U.sum(axis=1)). Then:
- W = W0 with diag(W) = diag(W0) − s, i.e. W = W0 − diag(s)
- So for any X: W @ X = -(U @ X + U.T @ X) - s[:, None] * X

Drop W construction and use WX computed as above:

- full:
```python
UX = U @ X
UTX = U.T @ X
s = -(U.sum(axis=0) + U.sum(axis=1))
WX = -(UX + UTX) - s[:, None] * X
dLdA = l * (X.T @ WX)
```

- diagonal:
```python
UX = U @ X
UTX = U.T @ X
s = -(U.sum(axis=0) + U.sum(axis=1))
WX = -(UX + UTX) - s[:, None] * X
dLdA = l * np.sum(X * WX, axis=0, keepdims=True).T
```

- decomposed:
```python
UX = U @ X
UTX = U.T @ X
s = -(U.sum(axis=0) + U.sum(axis=1))
WX = -(UX + UTX) - s[:, None] * X
dLdA = l * 2 * ((A @ X.T) @ WX)
```

This removes U+U.T and the large W temporary, reduces memory traffic, and works whether or not U is symmetric.

###################