Stochastic gradient descent with random batch selection based on selection and active coount
active / selection + dynamic epsilon
Add unit test for different modes of operation
Make functions composable for jax/jit
Sort data if animation = "all"

getters: dFdA, dFdE, A, E (only if converting to NumPy)
setters: dFdA, dFdE, A, E (only if converting from NumPy)

active_rows
active_cols
active_data
U
get_transformation_matrix() - self.A.shape[0]?
D
X
Y

* _A_psd_projection
* _A_centered_projection
* _E_positive_projection
* update_A
* update_E
  compute_A_0
  compute_E_0
* fun
* jac
* get_transformation_matrix (V)


substiture self.m in compute function
mask using boolean same class matrics (benchmark)
unfunc in_place, out, where, += -= *=
set and get functions for interacting with MLNNEngine
Scalars are python floats
__init__ and optimize should not need JAX
grid search unit test

func v grad shared computation
base class
intr(self, X)
func(self, X, I=None, full_output=False)
grad(self, X, I=None, full_output=False)
_intr(X, params)
_func(I, params)
_grad(I, params)


kernel=None | None, 'rbf', 'linear'
regularization='auto' | 'auto', 'unweighted'
init='auto' | 'auto', 'random', 'zero', 'identity', 'centered', 'pca', 'kpca'
callback=None | None, f(MLNNEngine, MLNNOptimizer, iter)
solver='steepest_fixed_backtracking' | 'steepest'/'bfgs', 'fixed'/'alternating', 'backtracking'/'strong_wolfe'
backend='numpy' | 'numpy', 'jax', 'cupy', 'numba'
verbose=0 | 0, 1, 2

Expand[Piecewise[{{0, x <= -1 / 2}, {2 / 3 * (x + 1 / 2) ^ 3, -1 / 2 < x <= 0}, {-2 / 3 * (x + 1 / 2) ^ 3 + 2 * (x + 1 / 2) ^ 2 - x - 1 / 3, 0 < x <= 1 / 2}, {x, 1 / 2 < x}}]]
Expand[D[Piecewise[{{0, x <= -1 / 2}, {2 / 3 * (x + 1 / 2) ^ 3, -1 / 2 < x <= 0}, {-2 / 3 * (x + 1 / 2) ^ 3 + 2 * (x + 1 / 2) ^ 2 - x - 1 / 3, 0 < x <= 1 / 2}, {x, 1 / 2 < x}}], x]]


